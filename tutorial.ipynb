{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow 'Crash Course'\n",
    "By JoyZ @ Oct 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Part 1: Intro of MLFlow\n",
    "* Homepage: https://mlflow.org/\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src=\"mlflow-modules.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "### Our Focus is **MLFlow Tracking**\n",
    "* Concept: https://mlflow.org/docs/latest/tracking.html#concepts\n",
    "\n",
    "### In this tutorial, we'll focus on utilising MLFlow Fluent API with:\n",
    "* Setting up a local MLFlow UI and experiment\n",
    "* Tracking the following aspects for your experiment runs:\n",
    "  * Parameters\n",
    "  * Metrics\n",
    "  * Artifacts (e.g. models, plots, datasets)\n",
    "  * Log information (e.g. tags and description)\n",
    "* Reusing the results of your runs\n",
    "* Managing your runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "## Part 2: Hands-on!\n",
    "\n",
    "### Setup a virtual environment for our project!\n",
    "```\n",
    "cd [project-folder]\n",
    "python3 -m venv venv/\n",
    "source venv/bin/activate\n",
    "which python3\n",
    "pip3 install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Modeling Data\n",
    "The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "\n",
    "Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wine-quality csv file from the URL\n",
    "csv_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(csv_url, sep=\";\")\n",
    "\n",
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "y = \"quality\"\n",
    "train_x = train.drop([y], axis=1)\n",
    "test_x = test.drop([y], axis=1)\n",
    "train_y = train[[y]]\n",
    "test_y = test[[y]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "### Let's play with the MLFLow now!\n",
    "* * * \n",
    "### Very first but important step: get familiar with the Docs!\n",
    "* Documentation: https://mlflow.org/docs/latest/index.html\n",
    "* Github: https://github.com/mlflow/mlflow\n",
    "* Python API: https://www.mlflow.org/docs/latest/python_api/index.html\n",
    "* R API: https://www.mlflow.org/docs/latest/R-api.html\n",
    "\n",
    "### Spin up the Tracking UI\n",
    "The Tracking UI lets you visualize, search and compare runs, as well as download run artifacts or metadata for analysis in other tools.\n",
    "\n",
    "If you log runs to a local mlruns directory, run mlflow ui in the directory above it, and it loads the corresponding runs.\n",
    "\n",
    "Type the below in your CLI if your UI is running successfully:\n",
    "```\n",
    "mlflow ui\n",
    "```\n",
    "and view it at http://127.0.0.1:5000\n",
    "\n",
    "### Set up an experiment otherwise it will use 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.create_experiment(name='Crash Course Demo')\n",
    "mlflow.set_experiment(experiment_name=\"Crash Course Demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a simple run\n",
    "* Check what's there in the UI\n",
    "* Check the local folder *mlruns/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run(run_name=\"my-first-run\")\n",
    "mlflow.log_param(\"hello\", \"world\")\n",
    "mlflow.log_metric(\"score\", 100)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some real modeling\n",
    "\n",
    "* Use with statement in python -> context manager\n",
    "* Hint - A context manager usually takes care of setting up some resource, e.g. opening a connection, and automatically handles the clean up when we are done with it\n",
    "* No need to end your run with mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"n_estimators\": 100, \"max_depth\": 4}\n",
    "\n",
    "with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "\n",
    "    clf_rf = RandomForestRegressor(**params, random_state=42)\n",
    "    clf_rf.fit(train_x, train_y)\n",
    "\n",
    "    y_test_predicted = clf_rf.predict(test_x)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(test_y, y_test_predicted)\n",
    "    print(\"RMSE: %s\" % rmse)\n",
    "    print(\"MAE: %s\" % mae)\n",
    "    print(\"R2: %s\" % r2)\n",
    "    \n",
    "    metrics = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "    # # load a single parameter\n",
    "    # mlflow.log_param(\"n_estimators\", 100)\n",
    "    \n",
    "    # # load individual metric\n",
    "    # mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # # load a dict of parameters\n",
    "    mlflow.log_params(params)\n",
    "    # load a dict of metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # # log model using mlflow supported model flavor (check doc for more)\n",
    "    mlflow.sklearn.log_model(clf_rf, \"random-forest-model\")\n",
    "\n",
    "    # Get the run id\n",
    "    print(\"Run ID: \", run.info.run_id)\n",
    "    rf_run_id = run.info.run_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log to an existing run using run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_run_id = \"d6527a30db6141baadf2ca007b43c129\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=rf_run_id):\n",
    "    mlflow.log_metric(\"new_metric\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log artifacts\n",
    "* mlflow.log_artifact() logs a **local** file or directory as an artifact, optionally taking an artifact_path to place it in within the runâ€™s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way\n",
    "* https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "train.to_csv(\"data/train_data.csv\", index=False)\n",
    "test.to_csv(\"data/test_data.csv\", index=False)\n",
    "\n",
    "# plots\n",
    "fig = train.hist(figsize=(10, 10))\n",
    "plt.savefig(\"plot/train_distribution.png\", format=\"png\")\n",
    "fig = test.hist(figsize=(10, 10))\n",
    "plt.savefig(\"plot/test_distribution.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=rf_run_id):\n",
    "    # a single file\n",
    "    mlflow.log_artifact(\"data/train_data.csv\", artifact_path=\"data\")\n",
    "    mlflow.log_artifact(\"plot/train_distribution.png\", artifact_path=\"plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=rf_run_id):\n",
    "    mlflow.log_artifacts(\"data\", artifact_path=\"data\")\n",
    "    mlflow.log_artifacts(\"plot\", artifact_path=\"plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's track a parameter search run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": 4}\n",
    "\n",
    "# Searching the best n_estimators\n",
    "\n",
    "for n_estimators in [100, 200, 300]:\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"random-forest-ntrees-{n_estimators}\"):\n",
    "        print(f\"-----> Start training with ntrees = {n_estimators}\")\n",
    "        \n",
    "        params.update({\"n_estimators\": n_estimators})\n",
    "        \n",
    "        clf = RandomForestRegressor(**params, random_state=42)\n",
    "        clf.fit(train_x, train_y)\n",
    "\n",
    "        predicted_qualities = clf.predict(test_x)\n",
    "\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "        print(\"RMSE: %s\" % rmse)\n",
    "        print(\"MAE: %s\" % mae)\n",
    "        print(\"R2: %s\" % r2)\n",
    "        metrics = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a better way - use parent & child runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"random-forest-parent-ntrees\") as parent_run:\n",
    "\n",
    "    rf_params = {\"max_depth\": 4}\n",
    "    \n",
    "    for n_estimators in [100, 200, 300]:\n",
    "        with mlflow.start_run(run_name=f\"ntrees-{n_estimators}\", nested=True):\n",
    "            \n",
    "            print(f\"-----> Start training with ntrees = {n_estimators}\")\n",
    "            \n",
    "            rf_params.update({\"n_estimators\": n_estimators})\n",
    "\n",
    "            clf = RandomForestRegressor(**rf_params, random_state=42)\n",
    "            clf.fit(train_x, train_y)\n",
    "\n",
    "            predicted_qualities = clf.predict(test_x)\n",
    "\n",
    "            (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "            rf_metrics = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "            print(\"RMSE: %s\" % rmse)\n",
    "            print(\"MAE: %s\" % mae)\n",
    "            print(\"R2: %s\" % r2)\n",
    "\n",
    "            mlflow.log_params(rf_params)\n",
    "            mlflow.log_metrics(rf_metrics)\n",
    "            mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set tag and description to your run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_desc = 'This is a test run'\n",
    "with mlflow.start_run(run_id=\"d1e3382727c2415187f30f48c8f20f29\", description=my_desc):\n",
    "    mlflow.set_tag('algorithm', 'randomforest')\n",
    "    # mlflow.set_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy? Let's try the mlflow's autolog\n",
    "* Automatic logging allows you to log metrics, parameters, and models without the need for explicit log statements.\n",
    "* MLflow currently supports a list of popular models. Check their doc for newest list: https://mlflow.org/docs/latest/tracking.html#automatic-logging\n",
    "* Detailed doc for mlflow.sklearn.autolog(): https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.autolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid_params = {\n",
    "    \"learning_rate\": [0.02, 0.05, 0.1],\n",
    "    \"n_estimators\": [i for i in range(100, 501, 100)],\n",
    "    \"max_depth\": [i for i in range(2, 12, 2)],\n",
    "    \"subsample\": [\n",
    "        0.5,\n",
    "        0.6,\n",
    "        0.7,\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(gbm_grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFLow currently supported sklearn model and GridSearchCV or RnadomizedSearchCV\n",
    "mlflow.sklearn.autolog(silent=True, max_tuning_runs=5)\n",
    "\n",
    "clf = GradientBoostingRegressor(random_state=42)\n",
    "grid = RandomizedSearchCV(clf, gbm_grid_params, n_iter=10, cv=3, verbose=0)\n",
    "grid.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load back a model and predict some samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Where is the model saved? \n",
    "  * Local artifact stores: mlruns/[experiment_id]/[run_id]/artifacts/[model_artifact_path]\n",
    "  * Or copy it from your MLflow UI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.sklearn.load_model('mlruns/1/edd4ab5f328e4537bb7fcc12783b09d3/artifacts/model')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = test_x.copy()\n",
    "predictions = model.predict(test_x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up your MLFlow runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete a run from the active runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.delete_run(run_id='7d413df6bed24f7c9ce38b1eabec01ea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> Check its meta.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove it completely from the backend store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more advanced use cases:\n",
    "###  Let's watch this video if we still have time \n",
    "https://app.pluralsight.com/course-player?clipId=26623955-88a7-49da-895a-b9621cc5616b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34705536cce7370137f9c9826dd3a1e80a569ae4bd1789ed88d63d1643a59a61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
